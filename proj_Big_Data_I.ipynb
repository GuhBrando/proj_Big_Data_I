{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto BIG DATA I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo de projeto\n",
    "\n",
    "Como queremos demostrar que de fato a solução proposta traz uma melhora, foi solicitado implementar uma análise comparativa de resultados usando a antiga abordagem (usando pandas) e usando a nova proposta de solução (pyspark). Para isso, tome em consideração o seguinte:\n",
    "\n",
    "1. Escolha dois conjuntos de dados interessantes, sendo que um deles é pequeno (menos de 10.000 linhas) e o outro bem maior (acima de 1.000.000 linhas).\n",
    "\n",
    "2. Aplique todas as etapas de ETL nos dois conjuntos de dados usando pandas y pyspark. As etapas incluem: (1) Extração dos dados, por exemplo de um csv, (2) Tratamento dos dados (limpeza, alteração de nomes de colunas, criação de mais tabelas, transformação nas colunas, etc.), e, (3) Carregamento dos dados (salvar a transformação feita sobre os dados).\n",
    "\n",
    "3. Lembre que cada etapa tem que ser feita usando unicamente pandas ou pyspark.\n",
    "\n",
    "4. Como o objetivo é fazer uma análise comparativa, tome em consideração o tempo que demora cada etapa, para depois facilitar as comparações.\n",
    "\n",
    "Boa sorte e divirta-se!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import timeit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando arquivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando a sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"Pandas vs PySpark\").getOrCreate()\n",
    "\n",
    "# Lendo os datasets csv usando pandas\n",
    "df_Netflix_pandas = pd.read_csv(\"datasets/Netflix_movies.csv\")\n",
    "df_Fraud_pandas = pd.read_csv(\"datasets/Fraudulent_Transactions_Data.csv\")\n",
    "\n",
    "# Lendo os datasets csv usando PySpark\n",
    "df_Netflix_spark = spark.read.csv(\"datasets/Netflix_movies.csv\", header=True, inferSchema=True)\n",
    "df_Fraud_spark = spark.read.csv(\"datasets/Fraudulent_Transactions_Data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar o tempo de execução de algumas operações comuns usando pandas e PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular o tempo\n",
    "def print_time_difference(pandas_time, spark_time):\n",
    "    \"\"\"\n",
    "    Calcula a diferença percentual entre os tempos de execução do Pandas e do PySpark e imprime o resultado.\n",
    "\n",
    "    Args:\n",
    "    pandas_time (float): Tempo de execução do Pandas.\n",
    "    spark_time (float): Tempo de execução do PySpark.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    percentual_diff = (pandas_time - spark_time) / pandas_time * 100\n",
    "\n",
    "    print(\"Tempo de execução (Pandas): \", pandas_time)\n",
    "    print(\"Tempo de execução (PySpark): \", spark_time)\n",
    "    print(\"Diferença percentual: \", percentual_diff, \"%\")\n",
    "\n",
    "    if pandas_time < spark_time:\n",
    "        print(\"Pandas foi mais rápido.\")\n",
    "    else:\n",
    "        print(\"PySpark foi mais rápido.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas (Pandas):  3323\n",
      "Quantidade de linhas (PySpark):  3323\n",
      "Tempo de execução (Pandas):  0.004980199970304966\n",
      "Tempo de execução (PySpark):  0.06663289992138743\n",
      "Diferença percentual:  -1237.9563133748447 %\n",
      "Pandas foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "# Operação de contar a quantidade de linhas do dataset Netflix usando pandas\n",
    "start = timeit.default_timer()\n",
    "df_Netflix_pandas_lines = df_Netflix_pandas.count()[0]\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "# Operação de contar a quantidade de linhas do dataset Netflix usando PySpark\n",
    "start = timeit.default_timer()\n",
    "df_Netflix_spark_lines = df_Netflix_spark.count()\n",
    "spark_time = timeit.default_timer() - start\n",
    "\n",
    "print(\"Quantidade de linhas (Pandas): \", df_Netflix_pandas_lines)\n",
    "print(\"Quantidade de linhas (PySpark): \", df_Netflix_spark_lines)\n",
    "print_time_difference(pandas_time, spark_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas (Pandas):  6362620\n",
      "Quantidade de linhas (PySpark):  6362620\n",
      "Tempo de execução (Pandas):  1.728370700031519\n",
      "Tempo de execução (PySpark):  0.42230560001917183\n",
      "Diferença percentual:  75.56626017720211 %\n",
      "PySpark foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "# Operação de contar a quantidade de linhas do dataset Fraude usando pandas\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_pandas_lines = df_Fraud_pandas.count()[0]\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "# Operação de contar a quantidade de linhas do dataset Fraude usando PySpark\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_spark_lines = df_Fraud_spark.count()\n",
    "spark_time = timeit.default_timer() - start\n",
    "\n",
    "print(\"Quantidade de linhas (Pandas): \", df_Fraud_pandas_lines)\n",
    "print(\"Quantidade de linhas (PySpark): \", df_Fraud_spark_lines)\n",
    "print_time_difference(pandas_time, spark_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'movie_name', 'Duration', 'year', 'genre', 'director', 'actors', 'country', 'rating', 'enter_in_netflix']\n",
      "Tempo de execução (Pandas):  0.0012341999681666493\n",
      "Tempo de execução (PySpark):  0.20459259999915957\n",
      "Diferença percentual:  -16476.9409557734 %\n",
      "Pandas foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Contar Distintos - Netflix\n",
    "\n",
    "# Descoberta de uma Coluna com um valor de distintos alto\n",
    "print(list(df_Netflix_pandas.columns))\n",
    "\n",
    "# Operação de contar a quantidade de distintos na coluna Actors do Netflix usando Pandas\n",
    "\n",
    "start = timeit.default_timer()\n",
    "df_Netflix_pandas_unique = df_Netflix_pandas[\"actors\"].unique()\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "# Operação de contar a quantidade de distintos na coluna Actors do Netflix usando PySpark\n",
    "start = timeit.default_timer()\n",
    "df_Netflix_spark_unique = df_Netflix_spark.select(\"actors\").distinct().count()\n",
    "spark_time = timeit.default_timer() - start\n",
    "\n",
    "print_time_difference(pandas_time, spark_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']\n",
      "Tempo de execução (Pandas):  2.038435299997218\n",
      "Tempo de execução (PySpark):  0.0064168000826612115\n",
      "Diferença percentual:  99.68520952896223 %\n",
      "PySpark foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Contar Distintos - Fraude\n",
    "\n",
    "# Descoberta de uma Coluna com um valor de distintos alto\n",
    "print(list(df_Fraud_pandas.columns))\n",
    "\n",
    "# Realizar distintos na coluna \"nameOrig\"\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_pandas_lines = df_Fraud_pandas[\"nameOrig\"].unique()\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "# Operação de contar a quantidade de distintos do dataset Netflix usando PySpark\n",
    "\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_spark_lines = df_Fraud_spark.select(\"nameOrig\").distinct()\n",
    "spark_time = timeit.default_timer() - start\n",
    "\n",
    "print_time_difference(pandas_time, spark_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas Antigas: ['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']\n",
      "Novas colunas: ['passo', 'tipo', 'quantidade', 'nome_origem', 'balanço_antigo_origem', 'balanço_novo_origem', 'nome_destino', 'balanço_antigo_destino', 'balanço_novo_destino', 'e_fraude', 'flag_de_fraude']\n",
      "\n",
      "Colunas Antigas: ['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']\n",
      "Novas colunas: ['passo', 'tipo', 'quantidade', 'nome_origem', 'balanço_antigo_origem', 'balanço_novo_origem', 'nome_destino', 'balanço_antigo_destino', 'balanço_novo_destino', 'e_fraude', 'flag_de_fraude']\n",
      "\n",
      "\n",
      "Tempo de execução (Pandas):  0.0002367000561207533\n",
      "Tempo de execução (PySpark):  0.004725900012999773\n",
      "Diferença percentual:  -1896.577478878518 %\n",
      "Pandas foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "# Mudança do nome das Colunas para portugues - Pandas\n",
    "\n",
    "new_columns = [\"passo\", \"tipo\", \"quantidade\", \n",
    "                \"nome_origem\", \"balanço_antigo_origem\", \"balanço_novo_origem\", \n",
    "                \"nome_destino\", \"balanço_antigo_destino\", \"balanço_novo_destino\", \n",
    "                \"e_fraude\", \"flag_de_fraude\"]\n",
    "\n",
    "print(\"Colunas Antigas: \" + str(list(df_Fraud_pandas.columns)))\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_pandas.columns = new_columns\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "print(\"Novas colunas: \" + str(list(df_Fraud_pandas.columns)))\n",
    "\n",
    "# Mudança do nome das Colunas para portugues - PySpark\n",
    "\n",
    "print(\"\\nColunas Antigas: \" + str(list(df_Fraud_spark.columns)))\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_spark = df_Fraud_spark.toDF(*new_columns)\n",
    "spark_time = timeit.default_timer() - start\n",
    "print(\"Novas colunas: \" + str(list(df_Fraud_spark.columns)))\n",
    "\n",
    "print(\"\\n\")\n",
    "print_time_difference(pandas_time, spark_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de frequência de cada valor na coluna 'country' do dataset Netflix\n",
      "Tempo de execução (Pandas):  0.0009373000357300043\n",
      "Tempo de execução (PySpark):  0.00794579996727407\n",
      "Diferença percentual:  -747.7328138673956 %\n",
      "Pandas foi mais rápido.\n",
      "\n",
      "Contagem de frequência de cada valor na coluna 'nome_destino' do dataset Fraude\n",
      "Tempo de execução (Pandas):  2.9998164999997243\n",
      "Tempo de execução (PySpark):  0.029606400057673454\n",
      "Diferença percentual:  99.01305963022484 %\n",
      "PySpark foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "# Contagem de frequência de cada valor em uma determinada coluna:\n",
    "# Realizar a contagem de frequência de cada valor na coluna \"country\" do dataset Netflix\n",
    "\n",
    "start = timeit.default_timer()\n",
    "value_counts = df_Netflix_pandas[\"country\"].value_counts()\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "start = timeit.default_timer()\n",
    "value_counts = df_Netflix_spark.groupBy(\"country\").count()\n",
    "spark_time = timeit.default_timer() - start\n",
    "\n",
    "print(\"Contagem de frequência de cada valor na coluna 'country' do dataset Netflix\")\n",
    "print_time_difference(pandas_time, spark_time)\n",
    "\n",
    "# Realizar a contagem de frequência de cada valor na coluna \"nome_destino\" do dataset Fraude\n",
    "\n",
    "start = timeit.default_timer()\n",
    "value_counts = df_Fraud_pandas[\"nome_destino\"].value_counts()\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "start = timeit.default_timer()\n",
    "value_counts = df_Fraud_spark.groupBy(\"nome_destino\").count()\n",
    "spark_time = timeit.default_timer() - start\n",
    "\n",
    "print(\"\\nContagem de frequência de cada valor na coluna 'nome_destino' do dataset Fraude\")\n",
    "print_time_difference(pandas_time, spark_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "160a5c293c9be6884e79214014d8354828695b573d01e9234f323aef8f71f523"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
