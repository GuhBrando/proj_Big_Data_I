{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto BIG DATA I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo de projeto\n",
    "\n",
    "Como queremos demostrar que de fato a solução proposta traz uma melhora, foi solicitado implementar uma análise comparativa de resultados usando a antiga abordagem (usando pandas) e usando a nova proposta de solução (pyspark). Para isso, tome em consideração o seguinte:\n",
    "\n",
    "1. Escolha dois conjuntos de dados interessantes, sendo que um deles é pequeno (menos de 10.000 linhas) e o outro bem maior (acima de 1.000.000 linhas).\n",
    "\n",
    "2. Aplique todas as etapas de ETL nos dois conjuntos de dados usando pandas y pyspark. As etapas incluem: (1) Extração dos dados, por exemplo de um csv, (2) Tratamento dos dados (limpeza, alteração de nomes de colunas, criação de mais tabelas, transformação nas colunas, etc.), e, (3) Carregamento dos dados (salvar a transformação feita sobre os dados).\n",
    "\n",
    "3. Lembre que cada etapa tem que ser feita usando unicamente pandas ou pyspark.\n",
    "\n",
    "4. Como o objetivo é fazer uma análise comparativa, tome em consideração o tempo que demora cada etapa, para depois facilitar as comparações.\n",
    "\n",
    "Boa sorte e divirta-se!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import timeit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas vs PySpark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando arquivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando a sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"Pandas vs PySpark\").getOrCreate()\n",
    "\n",
    "# Lendo os datasets csv usando pandas\n",
    "df_Netflix_pandas = pd.read_csv(\"datasets/Netflix_movies.csv\")\n",
    "df_Fraud_pandas = pd.read_csv(\"datasets/Fraudulent_Transactions_Data.csv\")\n",
    "\n",
    "# Lendo os datasets csv usando PySpark\n",
    "df_Netflix_spark = spark.read.csv(\"datasets/Netflix_movies.csv\", header=True, inferSchema=True)\n",
    "df_Fraud_spark = spark.read.csv(\"datasets/Fraudulent_Transactions_Data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar o tempo de execução de algumas operações comuns usando pandas e PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas (Pandas):  3323\n",
      "Quantidade de linhas (PySpark):  3323\n",
      "Tempo de execução (Pandas):  4.600000102072954e-05\n",
      "Tempo de execução (PySpark):  0.06350290001137182\n",
      "Diferença percentual:  -137949.77957012376 %\n",
      "Pandas foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "# Operação de contar a quantidade de linhas do dataset Netflix usando pandas\n",
    "start = timeit.default_timer()\n",
    "df_Netflix_pandas_lines = df_Netflix_pandas.shape[0]\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "# Operação de contar a quantidade de linhas do dataset Netflix usando PySpark\n",
    "start = timeit.default_timer()\n",
    "df_Netflix_spark_lines = df_Netflix_spark.count()\n",
    "spark_time = timeit.default_timer() - start\n",
    "\n",
    "# Cálculo da diferença percentual\n",
    "percentual_diff = (pandas_time - spark_time) / pandas_time * 100\n",
    "\n",
    "print(\"Quantidade de linhas (Pandas): \", df_Netflix_pandas_lines)\n",
    "print(\"Quantidade de linhas (PySpark): \", df_Netflix_spark_lines)\n",
    "print(\"Tempo de execução (Pandas): \", pandas_time)\n",
    "print(\"Tempo de execução (PySpark): \", spark_time)\n",
    "print(\"Diferença percentual: \", percentual_diff, \"%\")\n",
    "\n",
    "if pandas_time < spark_time:\n",
    "    print(\"Pandas foi mais rápido.\")\n",
    "else:\n",
    "    print(\"PySpark foi mais rápido.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas (Pandas):  6362620\n",
      "Quantidade de linhas (PySpark):  6362620\n",
      "Tempo de execução (Pandas):  6.150000263005495e-05\n",
      "Tempo de execução (PySpark):  0.460375499969814\n",
      "Diferença percentual:  -748478.0167183753 %\n",
      "Pandas foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "# Operação de contar a quantidade de linhas do dataset Fraudulent Transactions usando pandas\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_pandas_lines = df_Fraud_pandas.shape[0]\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "# Operação de contar a quantidade de linhas do dataset Fraudulent Transactions usando PySpark\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_spark_lines = df_Fraud_spark.count()\n",
    "spark_time = timeit.default_timer() - start\n",
    "\n",
    "# Cálculo da diferença percentual\n",
    "percentual_diff = (pandas_time - spark_time) / pandas_time * 100\n",
    "\n",
    "print(\"Quantidade de linhas (Pandas): \", df_Fraud_pandas_lines)\n",
    "print(\"Quantidade de linhas (PySpark): \", df_Fraud_spark_lines)\n",
    "print(\"Tempo de execução (Pandas): \", pandas_time)\n",
    "print(\"Tempo de execução (PySpark): \", spark_time)\n",
    "print(\"Diferença percentual: \", percentual_diff, \"%\")\n",
    "\n",
    "if pandas_time < spark_time:\n",
    "    print(\"Pandas foi mais rápido.\")\n",
    "else:\n",
    "    print(\"PySpark foi mais rápido.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "160a5c293c9be6884e79214014d8354828695b573d01e9234f323aef8f71f523"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
