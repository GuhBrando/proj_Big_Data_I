{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto BIG DATA I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo de projeto\n",
    "\n",
    "Como queremos demostrar que de fato a solução proposta traz uma melhora, foi solicitado implementar uma análise comparativa de resultados usando a antiga abordagem (usando pandas) e usando a nova proposta de solução (pyspark). Para isso, tome em consideração o seguinte:\n",
    "\n",
    "1. Escolha dois conjuntos de dados interessantes, sendo que um deles é pequeno (menos de 10.000 linhas) e o outro bem maior (acima de 1.000.000 linhas).\n",
    "\n",
    "2. Aplique todas as etapas de ETL nos dois conjuntos de dados usando pandas y pyspark. As etapas incluem: (1) Extração dos dados, por exemplo de um csv, (2) Tratamento dos dados (limpeza, alteração de nomes de colunas, criação de mais tabelas, transformação nas colunas, etc.), e, (3) Carregamento dos dados (salvar a transformação feita sobre os dados).\n",
    "\n",
    "3. Lembre que cada etapa tem que ser feita usando unicamente pandas ou pyspark.\n",
    "\n",
    "4. Como o objetivo é fazer uma análise comparativa, tome em consideração o tempo que demora cada etapa, para depois facilitar as comparações.\n",
    "\n",
    "Boa sorte e divirta-se!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import timeit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando arquivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando a sessão do Spark\n",
    "spark = SparkSession.builder.appName(\"Pandas vs PySpark\").getOrCreate()\n",
    "\n",
    "# Lendo os datasets csv usando pandas\n",
    "df_Netflix_pandas = pd.read_csv(\"datasets/Netflix_movies.csv\")\n",
    "df_Fraud_pandas = pd.read_csv(\"datasets/Fraudulent_Transactions_Data.csv\")\n",
    "\n",
    "# Lendo os datasets csv usando PySpark\n",
    "df_Netflix_spark = spark.read.csv(\"datasets/Netflix_movies.csv\", header=True, inferSchema=True)\n",
    "df_Fraud_spark = spark.read.csv(\"datasets/Fraudulent_Transactions_Data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar o tempo de execução de algumas operações comuns usando pandas e PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular o tempo\n",
    "def print_time_difference(pandas_time, spark_time):\n",
    "    # Cálculo da diferença percentual\n",
    "    percentual_diff = (pandas_time - spark_time) / pandas_time * 100\n",
    "\n",
    "    print(\"Tempo de execução (Pandas): \", pandas_time)\n",
    "    print(\"Tempo de execução (PySpark): \", spark_time)\n",
    "    print(\"Diferença percentual: \", percentual_diff, \"%\")\n",
    "\n",
    "    if pandas_time < spark_time:\n",
    "        print(\"Pandas foi mais rápido.\")\n",
    "    else:\n",
    "        print(\"PySpark foi mais rápido.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas (Pandas):  3323\n",
      "Quantidade de linhas (PySpark):  3323\n",
      "Tempo de execução (Pandas):  0.007209400000192545\n",
      "Tempo de execução (PySpark):  0.3617692000002535\n",
      "Diferença percentual:  -4918.020917005459 %\n",
      "Pandas foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "# Operação de contar a quantidade de linhas do dataset Netflix usando pandas\n",
    "start = timeit.default_timer()\n",
    "df_Netflix_pandas_lines = df_Netflix_pandas.count()[0]\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "# Operação de contar a quantidade de linhas do dataset Netflix usando PySpark\n",
    "start = timeit.default_timer()\n",
    "df_Netflix_spark_lines = df_Netflix_spark.count()\n",
    "spark_time = timeit.default_timer() - start\n",
    "\n",
    "print(\"Quantidade de linhas (Pandas): \", df_Netflix_pandas_lines)\n",
    "print(\"Quantidade de linhas (PySpark): \", df_Netflix_spark_lines)\n",
    "print_time_difference(pandas_time, spark_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de linhas (Pandas):  6362620\n",
      "Quantidade de linhas (PySpark):  6362620\n",
      "Tempo de execução (Pandas):  0.7520202000000609\n",
      "Tempo de execução (PySpark):  0.6993921999996928\n",
      "Diferença percentual:  6.998216271366618 %\n",
      "PySpark foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "# Operação de contar a quantidade de linhas do dataset Netflix usando pandas\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_pandas_lines = df_Fraud_pandas.count()[0]\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "# Operação de contar a quantidade de linhas do dataset Netflix usando PySpark\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_spark_lines = df_Fraud_spark.count()\n",
    "spark_time = timeit.default_timer() - start\n",
    "\n",
    "print(\"Quantidade de linhas (Pandas): \", df_Fraud_pandas_lines)\n",
    "print(\"Quantidade de linhas (PySpark): \", df_Fraud_spark_lines)\n",
    "print_time_difference(pandas_time, spark_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'movie_name', 'Duration', 'year', 'genre', 'director', 'actors', 'country', 'rating', 'enter_in_netflix']\n",
      "Tempo de execução (Pandas):  0.0014642999999523454\n",
      "Tempo de execução (PySpark):  0.6993921999996928\n",
      "Diferença percentual:  -47662.903778081956 %\n",
      "Pandas foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Contar Distintos - Netflix\n",
    "\n",
    "# Descoberta de uma Coluna com um valor de distintos alto\n",
    "print(list(df_Netflix_pandas.columns))\n",
    "\n",
    "# Realizar distintos na coluna Actors\n",
    "start = timeit.default_timer()\n",
    "df_Netflix_pandas_lines = df_Netflix_pandas[\"actors\"].unique()\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "# Operação de contar a quantidade de distintos do dataset Netflix usando PySpark\n",
    "\n",
    "# -------------\n",
    "\n",
    "print_time_difference(pandas_time, spark_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']\n",
      "Tempo de execução (Pandas):  2.1959633999999824\n",
      "Tempo de execução (PySpark):  0.006561199999850942\n",
      "Diferença percentual:  99.7012154209924 %\n",
      "PySpark foi mais rápido.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Contar Distintos - Fraude\n",
    "\n",
    "# Descoberta de uma Coluna com um valor de distintos alto\n",
    "print(list(df_Fraud_pandas.columns))\n",
    "\n",
    "# Realizar distintos na coluna \"nameOrig\"\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_pandas_lines = df_Fraud_pandas[\"nameOrig\"].unique()\n",
    "pandas_time = timeit.default_timer() - start\n",
    "\n",
    "# Operação de contar a quantidade de distintos do dataset Netflix usando PySpark\n",
    "\n",
    "start = timeit.default_timer()\n",
    "df_Fraud_spark_lines = df_Fraud_spark.select(\"nameOrig\").distinct()\n",
    "spark_time = timeit.default_timer() - start\n",
    "\n",
    "print_time_difference(pandas_time, spark_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas Antigas: ['passo', 'tipo', 'quantidade', 'nome_origem', 'balanço_antigo_origem', 'balanço_novo_origem', 'nome_destino', 'balanço_antigo_destino', 'balanço_novo_destino', 'e_fraude', 'flag_de_fraude']\n",
      "Novas colunas: ['passo', 'tipo', 'quantidade', 'nome_origem', 'balanço_antigo_origem', 'balanço_novo_origem', 'nome_destino', 'balanço_antigo_destino', 'balanço_novo_destino', 'e_fraude', 'flag_de_fraude']\n",
      "\n",
      "Colunas Antigas: ['step', 'type', 'amount', 'nameOrig', 'oldbalanceOrg', 'newbalanceOrig', 'nameDest', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']\n",
      "Novas colunas: ['passo', 'tipo', 'quantidade', 'nome_origem', 'balanço_antigo_origem', 'balanço_novo_origem', 'nome_destino', 'balanço_antigo_destino', 'balanço_novo_destino', 'e_fraude', 'flag_de_fraude']\n"
     ]
    }
   ],
   "source": [
    "# Mudança do nome das Colunas - Pandas\n",
    "\n",
    "new_columns = [\"passo\", \"tipo\", \"quantidade\", \n",
    "                \"nome_origem\", \"balanço_antigo_origem\", \"balanço_novo_origem\", \n",
    "                \"nome_destino\", \"balanço_antigo_destino\", \"balanço_novo_destino\", \n",
    "                \"e_fraude\", \"flag_de_fraude\"]\n",
    "\n",
    "print(\"Colunas Antigas: \" + str(list(df_Fraud_pandas.columns)))\n",
    "df_Fraud_pandas.columns = new_columns\n",
    "print(\"Novas colunas: \" + str(list(df_Fraud_pandas.columns)))\n",
    "\n",
    "# Mudança do nome das Colunas - Spark\n",
    "\n",
    "print(\"\\nColunas Antigas: \" + str(list(df_Fraud_spark.columns)))\n",
    "df_Fraud_spark = df_Fraud_spark.toDF(*new_columns)\n",
    "print(\"Novas colunas: \" + str(list(df_Fraud_spark.columns)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d0a57d5b68fdad216000040c9816578f8ed20457dd7023f1092dca2c1399509"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
