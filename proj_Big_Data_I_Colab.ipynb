{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt3_zry5EWOg"
      },
      "source": [
        "# Projeto BIG DATA I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKE2hxF2Ea5O"
      },
      "source": [
        "## Objetivo de projeto\n",
        "\n",
        "Como queremos demostrar que de fato a solução proposta traz uma melhora, foi solicitado implementar uma análise comparativa de resultados usando a antiga abordagem (usando pandas) e usando a nova proposta de solução (pyspark). Para isso, tome em consideração o seguinte:\n",
        "\n",
        "1. Escolha dois conjuntos de dados interessantes, sendo que um deles é pequeno (menos de 10.000 linhas) e o outro bem maior (acima de 1.000.000 linhas).\n",
        "\n",
        "2. Aplique todas as etapas de ETL nos dois conjuntos de dados usando pandas y pyspark. As etapas incluem: (1) Extração dos dados, por exemplo de um csv, (2) Tratamento dos dados (limpeza, alteração de nomes de colunas, criação de mais tabelas, transformação nas colunas, etc.), e, (3) Carregamento dos dados (salvar a transformação feita sobre os dados).\n",
        "\n",
        "3. Lembre que cada etapa tem que ser feita usando unicamente pandas ou pyspark.\n",
        "\n",
        "4. Como o objetivo é fazer uma análise comparativa, tome em consideração o tempo que demora cada etapa, para depois facilitar as comparações.\n",
        "\n",
        "Boa sorte e divirta-se!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWo6nfR2qyvJ"
      },
      "source": [
        "## Miscelâneas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjvOeKsBEkbE"
      },
      "source": [
        "### Instalando o PySpark no Google Colab\n",
        "Instalar o PySpark não é um processo direto como de praxe em Python. Não basta usar um pip install apenas. Na verdade, antes de tudo é necessário instalar dependências como o **Java 8**, **Apache Spark 2.3.2** junto com o **Hadoop 2.7**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlUEgcJGEt6k"
      },
      "outputs": [],
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
        "\n",
        "# Unzip the file\n",
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz\n",
        "\n",
        "# Install library for finding Spark\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lemSyZz_HMkM"
      },
      "source": [
        "A próxima etapa é configurar as variáveis de ambiente, pois isso habilita o ambiente do Colab a identificar corretamente onde as dependências estão rodando.\n",
        "\n",
        "Para conseguir “manipular” o terminal e interagir como ele, você pode usar a biblioteca os."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTExanCAIdK1"
      },
      "source": [
        "### Set up the environment for Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0bMCtBxIcMM"
      },
      "outputs": [],
      "source": [
        "# Configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTxERahtIXEr"
      },
      "source": [
        "### Import the library for locating Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KXscQjEHLIT",
        "outputId": "d4c04951-14df-4dd5-9b68-8f8226399d3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.2.3-bin-hadoop3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import findspark  # Initiate findspark\n",
        "\n",
        "findspark.init()  # Check the location for Spark\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzTK7ZR1Is6v"
      },
      "source": [
        "### Start a Spark session, and check the session information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4Xc1CXsH0XX",
        "outputId": "7d2116f7-0505-4a9e-d1ec-180d87f38095"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f2a1c2a7280>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://06f9e06ef413:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()# Check Spark Session Information\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFV2m8_nK4ET"
      },
      "source": [
        "## Importar arquivos csv do GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGEmF0bmlLiU",
        "outputId": "ad8c46b3-6b34-4b26-f692-36b6029b5b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-13 22:48:12--  https://github.com/GuhBrando/proj_Big_Data_I/raw/main/datasets/Fraudulent_Transactions_Data.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/GuhBrando/proj_Big_Data_I/main/datasets/Fraudulent_Transactions_Data.csv [following]\n",
            "--2023-02-13 22:48:13--  https://media.githubusercontent.com/media/GuhBrando/proj_Big_Data_I/main/datasets/Fraudulent_Transactions_Data.csv\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 493534783 (471M) [text/plain]\n",
            "Saving to: ‘Fraudulent_Transactions_Data.csv’\n",
            "\n",
            "Fraudulent_Transact 100%[===================>] 470.67M   216MB/s    in 2.2s    \n",
            "\n",
            "2023-02-13 22:48:32 (216 MB/s) - ‘Fraudulent_Transactions_Data.csv’ saved [493534783/493534783]\n",
            "\n",
            "--2023-02-13 22:48:33--  https://github.com/GuhBrando/proj_Big_Data_I/raw/main/datasets/Netflix_movies.csv\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GuhBrando/proj_Big_Data_I/main/datasets/Netflix_movies.csv [following]\n",
            "--2023-02-13 22:48:33--  https://raw.githubusercontent.com/GuhBrando/proj_Big_Data_I/main/datasets/Netflix_movies.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 782174 (764K) [text/plain]\n",
            "Saving to: ‘Netflix_movies.csv’\n",
            "\n",
            "Netflix_movies.csv  100%[===================>] 763.84K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-02-13 22:48:33 (134 MB/s) - ‘Netflix_movies.csv’ saved [782174/782174]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/GuhBrando/proj_Big_Data_I/raw/main/datasets/Fraudulent_Transactions_Data.csv\n",
        "!wget https://github.com/GuhBrando/proj_Big_Data_I/raw/main/datasets/Netflix_movies.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuwIVx8hVeDF"
      },
      "source": [
        "## Pandas vs PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zZ-W-euZynJ"
      },
      "source": [
        "### Importando bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQglRjGgVjHI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pandas import DataFrame\n",
        "from pyspark.sql import DataFrame\n",
        "import time\n",
        "\n",
        "# Gerenciamento de arquivos\n",
        "import os\n",
        "import shutil\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez9t5-Y6Z1ri"
      },
      "source": [
        "### Comparação de tempos para leitura dos arquivos csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW87_OVLnL0D",
        "outputId": "eab1a3ad-fe4e-4191-b6b4-57a48f7488af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de leitura dos arquivos csv com Pandas: 10.6843 segundos\n",
            "Tempo de leitura dos arquivos csv com PySpark: 27.828 segundos\n",
            "Pandas foi mais rápido na leitura dos arquivos csv\n",
            "\n",
            "\n",
            "Tempo de conversão dos arquivos para Parquet: 37.6018 segundos\n",
            "Tempo de leitura dos arquivos Parquet com Spark: 0.3337 segundos\n",
            "A biblioteca com menor tempo de processamento é o Spark com Parquet\n"
          ]
        }
      ],
      "source": [
        "#definições\n",
        "num_casas = 4\n",
        "\n",
        "# Iniciando a sessão do Spark\n",
        "spark = SparkSession.builder.appName(\"Pandas_vs_PySpark\").getOrCreate()\n",
        "\n",
        "# Medindo o tempo de leitura dos arquivos csv usando pandas\n",
        "start_time = time.time()\n",
        "df_Netflix_pandas = pd.read_csv(\"Netflix_movies.csv\")\n",
        "df_Fraud_pandas = pd.read_csv(\"Fraudulent_Transactions_Data.csv\")\n",
        "pandas_time = time.time() - start_time\n",
        "\n",
        "# Medindo o tempo de leitura dos arquivos csv usando PySpark\n",
        "start_time = time.time()\n",
        "df_Netflix_spark = spark.read.csv(\"Netflix_movies.csv\", header=True, inferSchema=True)\n",
        "df_Fraud_spark = spark.read.csv(\"Fraudulent_Transactions_Data.csv\", header=True, inferSchema=True)\n",
        "pyspark_time = time.time() - start_time\n",
        "\n",
        "# Exibindo os tempos de execução\n",
        "print(\"Tempo de leitura dos arquivos csv com Pandas: %s segundos\" % round(pandas_time, num_casas))\n",
        "print(\"Tempo de leitura dos arquivos csv com PySpark: %s segundos\" % round(pyspark_time, num_casas))\n",
        "\n",
        "# Verificando qual foi mais rápido\n",
        "if pandas_time < pyspark_time:\n",
        "    print(\"Pandas foi mais rápido na leitura dos arquivos csv\")\n",
        "else:\n",
        "    print(\"PySpark foi mais rápido na leitura dos arquivos csv\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Conversão para Parquet\n",
        "# Verifica e exclui arquivos parquet anteriores\n",
        "\n",
        "if os.path.exists('Netflix_movies.parquet'):\n",
        "  shutil.rmtree('Netflix_movies.parquet')\n",
        "if os.path.exists('Fraudulent_Transactions_Data.parquet'):\n",
        "  shutil.rmtree('Fraudulent_Transactions_Data.parquet')\n",
        "\n",
        "# Convertendo os arquivos csv para Parquet\n",
        "start_time = time.time()\n",
        "df_Netflix_spark.write.parquet('Netflix_movies.parquet')\n",
        "df_Fraud_spark.write.parquet('Fraudulent_Transactions_Data.parquet')\n",
        "convert_parquet_time = time.time() - start_time\n",
        "\n",
        "# Exibindo o tempo de execução\n",
        "print(\"Tempo de conversão dos arquivos para Parquet: %s segundos\" % round(convert_parquet_time, num_casas))\n",
        "\n",
        "# Iniciando a contagem de tempo para leitura do arquivo Parquet com Spark\n",
        "start_time_parquet = time.time()\n",
        "df_Netflix_parquet = spark.read.parquet('Netflix_movies.parquet')\n",
        "df_Fraud_parquet = spark.read.parquet('Fraudulent_Transactions_Data.parquet')\n",
        "parquet_time = time.time() - start_time_parquet\n",
        "\n",
        "# Exibindo o tempo de execução\n",
        "print(\"Tempo de leitura dos arquivos Parquet com Spark: %s segundos\" % round(parquet_time, num_casas))\n",
        "\n",
        "min_time = min(pandas_time, pyspark_time, parquet_time)\n",
        "\n",
        "# Verificando qual foi mais rápido\n",
        "if min_time == pandas_time:\n",
        "  print(\"A biblioteca com menor tempo de processamento é o Pandas\")\n",
        "elif min_time == pyspark_time:\n",
        "  print(\"A biblioteca com menor tempo de processamento é o PySpark\")\n",
        "else:\n",
        "  print(\"A biblioteca com menor tempo de processamento é o Spark com Parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Hj65qLV0X2"
      },
      "source": [
        "### Comparar o tempo de execução de algumas operações comuns usando pandas e PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1UVmb-RaBVM"
      },
      "source": [
        "- Soma de coluna e output de valor unico\n",
        "- Somas das linhas e inserir em uma nova coluna\n",
        "- Schema estrela - em uma coluna\n",
        "- Trocar nome das Colunas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsfQlEkOz5G-"
      },
      "source": [
        "###  Contando a quantidade de linhas do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpV3_bpyE7-l",
        "outputId": "acbf294d-9b33-48b9-94a4-ac7b2f09f390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de linhas do dataset Netflix_Movie_Data usando pandas: 3323\n",
            "Quantidade de linhas do dataset Netflix_Movie_Data usando PySpark: 3323\n",
            "Quantidade de linhas do dataset Netflix_Movie_Data usando Spark: 3323\n",
            "\n",
            "\n",
            "Tempo de contar as linhas do dataset Netflix_Movie_Data usando pandas: 0.00475 segundos\n",
            "Tempo de contar as linhas do dataset Netflix_Movie_Data usando PySpark: 1.19152 segundos\n",
            "Tempo de contar as linhas do dataset Netflix_Movie_Data usando Spark: 1.08273 segundos\n",
            "\n",
            "\n",
            "A biblioteca com menor tempo de processamento para o dataset Netflix_Movie_Data é o Pandas\n",
            "\n",
            "\n",
            "Quantidade de linhas do dataset Fraudulent_Transactions_Data usando pandas: 6362620\n",
            "Quantidade de linhas do dataset Fraudulent_Transactions_Data usando PySpark: 6362620\n",
            "Quantidade de linhas do dataset Fraudulent_Transactions_Data usando Spark: 6362620\n",
            "\n",
            "\n",
            "Tempo de contar as linhas do dataset Fraudulent_Transactions_Data usando pandas: 1.49131 segundos\n",
            "Tempo de contar as linhas do dataset Fraudulent_Transactions_Data usando PySpark: 6.25629 segundos\n",
            "Tempo de contar as linhas do dataset Fraudulent_Transactions_Data usando Spark: 0.23984 segundos\n",
            "\n",
            "\n",
            "A biblioteca com menor tempo de processamento para o dataset Fraudulent_Transactions_Data é o Spark com Parquet\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Função para contar o número de linhas e tempo de processamento de três bibliotecas diferentes\n",
        "def count_rows_and_time(dataset, df_pandas, df_spark, df_parquet):\n",
        "    \"\"\"\n",
        "    Função que compara o tempo de processamento entre as bibliotecas Pandas, PySpark e Spark com Parquet ao contar as linhas de um determinado dataset.\n",
        "\n",
        "    Parâmetros:\n",
        "    dataset (str) : Nome do dataset a ser avaliado.\n",
        "    df_pandas (pandas.DataFrame) : DataFrame do Pandas para o dataset informado.\n",
        "    df_spark (pyspark.sql.DataFrame) : DataFrame do PySpark para o dataset informado.\n",
        "    df_parquet (pyspark.sql.DataFrame) : DataFrame do Spark com Parquet para o dataset informado.\n",
        "\n",
        "    Saídas:\n",
        "    Imprime na tela o número de linhas de cada biblioteca para o dataset informado e o tempo de processamento de contar as linhas. Além disso, imprime a biblioteca com menor tempo de processamento para o dataset informado.\n",
        "    \"\"\"\n",
        "    #numero de casas de arredondamento\n",
        "    num_casas = 5\n",
        "    \n",
        "    start_time = time.time()\n",
        "    pandas_rows = df_pandas[df_pandas.columns[1]].count()\n",
        "    pandas_count_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    pyspark_rows = df_spark.count()\n",
        "    pyspark_count_time = time.time() - start_time\n",
        "    \n",
        "    start_time = time.time()\n",
        "    spark_rows = df_parquet.count()\n",
        "    spark_count_time = time.time() - start_time\n",
        "\n",
        "    # Exibindo as informações\n",
        "    print(\"Quantidade de linhas do dataset %s usando pandas: %s\" % (dataset, pandas_rows))\n",
        "    print(\"Quantidade de linhas do dataset %s usando PySpark: %s\" % (dataset, pyspark_rows))\n",
        "    print(\"Quantidade de linhas do dataset %s usando Spark: %s\" % (dataset, spark_rows))\n",
        "    print(\"\\n\")\n",
        "    print(\"Tempo de contar as linhas do dataset %s usando pandas: %s segundos\" % (dataset, round(pandas_count_time, num_casas)))\n",
        "    print(\"Tempo de contar as linhas do dataset %s usando PySpark: %s segundos\" % (dataset, round(pyspark_count_time, num_casas)))\n",
        "    print(\"Tempo de contar as linhas do dataset %s usando Spark: %s segundos\" % (dataset, round(spark_count_time, num_casas)))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Encontrando a biblioteca com menor tempo de processamento\n",
        "    min_time = min(pandas_count_time, pyspark_count_time, spark_count_time)\n",
        "    if min_time == pandas_count_time:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o Pandas\" % dataset)\n",
        "    elif min_time == pyspark_count_time:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o PySpark\" % dataset)\n",
        "    else:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o Spark com Parquet\" % dataset)\n",
        "    print(\"\\n\")\n",
        "\n",
        "# aplicando a função para Netflix_Movie_Data\n",
        "count_rows_and_time(\"Netflix_Movie_Data\", df_Netflix_pandas, df_Netflix_spark, df_Netflix_parquet)\n",
        "\n",
        "# aplicando a função para Fraudulent_Transactions_Data\n",
        "count_rows_and_time(\"Fraudulent_Transactions_Data\", df_Fraud_pandas, df_Fraud_spark, df_Fraud_parquet)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myi3nBGR0slh"
      },
      "source": [
        "### Renomear colunas dos Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RdecgtdEH0-"
      },
      "source": [
        "#### Dicionário e Funções:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1P6uUWq4Zg_"
      },
      "outputs": [],
      "source": [
        "# Nome para as colunas dos datasets\n",
        "TRANSACTION_COL_DICT = {\n",
        "    \"step\": \"time_step\",\n",
        "    \"type\": \"transaction_type\",\n",
        "    \"amount\": \"amount\",\n",
        "    \"nameOrig\": \"originating_customer\",\n",
        "    \"oldbalanceOrg\": \"originating_balance_before\",\n",
        "    \"newbalanceOrig\": \"originating_balance_after\",\n",
        "    \"nameDest\": \"recipient_customer\",\n",
        "    \"oldbalanceDest\": \"recipient_balance_before\",\n",
        "    \"newbalanceDest\": \"recipient_balance_after\",\n",
        "    \"isFraud\": \"is_fraud\",\n",
        "    \"isFlaggedFraud\": \"is_flagged_fraud\"\n",
        "}\n",
        "\n",
        "NETFLIX_MOVIE_COL_DICT = {\n",
        "\"Unnamed: 0\": \"index\",\n",
        "\"movie_name\": \"movie_name\",\n",
        "\"Duration\": \"duration\",\n",
        "\"year\": \"year_of_production\",\n",
        "\"genre\": \"genre\",\n",
        "\"director\": \"director\",\n",
        "\"actors\": \"actors\",\n",
        "\"country\": \"country_of_production\",\n",
        "\"rating\": \"rating\",\n",
        "\"enter_in_netflix\": \"date_of_entry\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-geF0cQL51Cz"
      },
      "outputs": [],
      "source": [
        "# Função para renomear colunas com Pandas\n",
        "def rename_cols_pandas(df: DataFrame, mapping_dict: dict) -> DataFrame:\n",
        "\n",
        "    ''' \n",
        "    Função para renomear colunas de um dataframe usando a biblioteca Pandas.\n",
        "\n",
        "    :param df: DataFrame de entrada\n",
        "    :param mapping_dict: Dicionário com o mapeamento dos nomes das colunas\n",
        "    :return: DataFrame de saída com as colunas renomeadas\n",
        "    ''' \n",
        "    \n",
        "    df.rename(columns=mapping_dict, inplace=True)\n",
        "    return df\n",
        "    \n",
        "# Função para renomear colunas com PySpark\n",
        "def rename_cols_pyspark(df: DataFrame, mapping_dict: dict) ->DataFrame:\n",
        "    # Rename all the columns\n",
        "    '''\n",
        "    Função para renomear colunas de um dataframe usando a biblioteca PySpark.\n",
        "\n",
        "    :param df: DataFrame de entrada\n",
        "    :param mapping_dict: Dicionário com o mapeamento dos nomes das colunas\n",
        "    :return: DataFrame de saída com as colunas renomeadas\n",
        "    '''\n",
        "    for key in mapping_dict.keys():\n",
        "        df=df.withColumnRenamed(key,mapping_dict.get(key))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRuyN5NBEQor"
      },
      "source": [
        "#### Comparações de tempo ao Renomear colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qv0wlLh6Eab",
        "outputId": "e6dc50ee-c103-4917-c3a7-40e9b7cfa84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo de renomear as colunas do dataset Netflix_Movie_Data usando Pandas: 0.0012 segundos\n",
            "Tempo de renomear as colunas do dataset Netflix_Movie_Data usando PySpark: 0.1663 segundos\n",
            "Tempo de renomear as colunas do dataset Netflix_Movie_Data usando Spark com Parquet: 0.1278 segundos\n",
            "A biblioteca com menor tempo de processamento para o dataset Netflix_Movie_Data é o Pandas\n",
            "\n",
            "\n",
            "Tempo de renomear as colunas do dataset Fraudulent_Transactions_Data usando Pandas: 0.0006 segundos\n",
            "Tempo de renomear as colunas do dataset Fraudulent_Transactions_Data usando PySpark: 0.1661 segundos\n",
            "Tempo de renomear as colunas do dataset Fraudulent_Transactions_Data usando Spark com Parquet: 0.0849 segundos\n",
            "A biblioteca com menor tempo de processamento para o dataset Fraudulent_Transactions_Data é o Pandas\n"
          ]
        }
      ],
      "source": [
        "# 1. Rename Columns pandas\n",
        "\n",
        "#definições\n",
        "num_casas = 4\n",
        "\n",
        "# Pandas\n",
        "start_time = time.time()\n",
        "df_Netflix_pandas = rename_cols_pandas(df_Netflix_pandas, NETFLIX_MOVIE_COL_DICT)\n",
        "pandas_rename_time_Netflix = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "df_Fraud_pandas = rename_cols_pandas(df_Fraud_pandas, TRANSACTION_COL_DICT)\n",
        "pandas_rename_time_Fraud = time.time() - start_time\n",
        "\n",
        "# PySpark csv\n",
        "start_time = time.time()\n",
        "df_Netflix_spark = rename_cols_pyspark(df_Netflix_spark, NETFLIX_MOVIE_COL_DICT)\n",
        "pyspark_rename_time_Netflix = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "df_Fraud_spark = rename_cols_pyspark(df_Fraud_spark, TRANSACTION_COL_DICT)\n",
        "pyspark_time_Fraud = time.time() - start_time\n",
        "\n",
        "# PySpark parquet\n",
        "start_time = time.time()\n",
        "df_Netflix_parquet = rename_cols_pyspark(df_Netflix_parquet, NETFLIX_MOVIE_COL_DICT)\n",
        "spark_rename_time_Netflix = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "df_Fraud_parquet = rename_cols_pyspark(df_Fraud_parquet, TRANSACTION_COL_DICT)\n",
        "spark_time_Fraud = time.time() - start_time\n",
        "\n",
        "# Exibindo os tempos de renomeação de colunas\n",
        "print(\"Tempo de renomear as colunas do dataset Netflix_Movie_Data usando Pandas: %s segundos\" % round(pandas_rename_time_Netflix, num_casas))\n",
        "print(\"Tempo de renomear as colunas do dataset Netflix_Movie_Data usando PySpark: %s segundos\" % round(pyspark_rename_time_Netflix, num_casas))\n",
        "print(\"Tempo de renomear as colunas do dataset Netflix_Movie_Data usando Spark com Parquet: %s segundos\" % round(spark_rename_time_Netflix, num_casas))\n",
        "\n",
        "# Verificando qual foi mais rápido para o dataset Netflix_Movie_Data\n",
        "min_time = min(pandas_rename_time_Netflix, pyspark_rename_time_Netflix, spark_rename_time_Netflix)\n",
        "\n",
        "if min_time == pandas_rename_time_Netflix:\n",
        "  print(\"A biblioteca com menor tempo de processamento para o dataset Netflix_Movie_Data é o Pandas\")\n",
        "elif min_time == pyspark_rename_time_Netflix:\n",
        "  print(\"A biblioteca com menor tempo de processamento para o dataset Netflix_Movie_Data é o PySpark\")\n",
        "else:\n",
        "  print(\"A biblioteca com menor tempo de processamento para o dataset Netflix_Movie_Data é o Spark com Parquet\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Tempo de renomear as colunas do dataset Fraudulent_Transactions_Data usando Pandas: %s segundos\" % round(pandas_rename_time_Fraud, num_casas))\n",
        "print(\"Tempo de renomear as colunas do dataset Fraudulent_Transactions_Data usando PySpark: %s segundos\" % round(pyspark_time_Fraud, num_casas))\n",
        "print(\"Tempo de renomear as colunas do dataset Fraudulent_Transactions_Data usando Spark com Parquet: %s segundos\" %round(spark_time_Fraud, num_casas))\n",
        "# Verificando qual foi mais rápido para o dataset Fraudulent_Transactions_Data\n",
        "min_time = min(pandas_rename_time_Fraud, pyspark_time_Fraud, spark_time_Fraud)\n",
        "\n",
        "if min_time == pandas_rename_time_Fraud:\n",
        "  print(\"A biblioteca com menor tempo de processamento para o dataset Fraudulent_Transactions_Data é o Pandas\")\n",
        "elif min_time == pyspark_time_Fraud:\n",
        "  print(\"A biblioteca com menor tempo de processamento para o dataset Fraudulent_Transactions_Data é o PySpark\")\n",
        "else:\n",
        "  print(\"A biblioteca com menor tempo de processamento para o dataset Fraudulent_Transactions_Data é o Spark com Parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cshXXNrPxBgl"
      },
      "source": [
        "###Soma de coluna e output de valor unico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-fiWBdlx2HZ"
      },
      "source": [
        "Nesta etapa são comparadas o tempo de cálculo de um valor de coluna. \n",
        "\n",
        "- Para os dados do Netflix_movies vamos calcular a média do rating de todos os filmes. \n",
        "- Para os dados do Fraudulent_Transaction_Data vamos calcular o valor médio das transações fraudulentas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQcB8s_sdTXd"
      },
      "source": [
        "#### Rating medio do Netflix_Movie_Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpsaBGn4y3q_",
        "outputId": "15193b8b-91fc-40a1-ce4f-76f1046ae5d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rating medio no Netflix_Movie_Data usando pandas: 3323\n",
            "Rating medio no Netflix_Movie_Data usando pandas: 6.20093\n",
            "Rating medio no Netflix_Movie_Data usando pandas: 6.20093\n",
            "\n",
            "\n",
            "Tempo de calculo da médias do dataset Netflix_Movie_Data usando pandas: 0.00034 segundos\n",
            "Tempo de calculo da médias do dataset Netflix_Movie_Data usando PySpark: 0.42063 segundos\n",
            "Tempo de calculo da médias do dataset Netflix_Movie_Data usando Spark com Parquet: 0.29307 segundos\n",
            "\n",
            "\n",
            "A biblioteca com menor tempo de processamento para o dataset Netflix_Movie_Data é o Pandas\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "# Função para calcular o valor médio do ranting \n",
        "def mean_rating_time(dataset, df_pandas, df_spark, df_parquet):\n",
        "    \"\"\"\n",
        "    Função que compara o tempo de processamento entre as bibliotecas Pandas, PySpark e Spark com Parquet ao contar as linhas de um determinado dataset.\n",
        "\n",
        "    Parâmetros:\n",
        "    dataset (str) : Nome do dataset a ser avaliado.\n",
        "    df_pandas (pandas.DataFrame) : DataFrame do Pandas para o dataset informado.\n",
        "    df_spark (pyspark.sql.DataFrame) : DataFrame do PySpark para o dataset informado.\n",
        "    df_parquet (pyspark.sql.DataFrame) : DataFrame do Spark com Parquet para o dataset informado.\n",
        "\n",
        "    Saídas:\n",
        "    Imprime na tela o número médio de cada biblioteca para o dataset informado e o tempo de processamento de contar as linhas. Além disso, imprime a biblioteca com menor tempo de processamento para o dataset informado.\n",
        "    \"\"\"\n",
        "    #numero de casas de arredondamento\n",
        "    num_casas = 5\n",
        "    \n",
        "    start_time = time.time()\n",
        "    pandas_mean = df_pandas[df_pandas.columns[8]].count()\n",
        "    pandas_mean_time = time.time() - start_time\n",
        "    \n",
        "    start_time = time.time()\n",
        "    pyspark_mean = df_spark.select(mean('rating')).collect()\n",
        "    results={}\n",
        "    for i in pyspark_mean:\n",
        "      results.update(i.asDict())\n",
        "    pyspark_mean = results['avg(rating)']\n",
        "    pyspark_mean_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    parquet_mean = df_parquet.select(mean('rating')).collect()\n",
        "    results={}\n",
        "    for i in parquet_mean:\n",
        "      results.update(i.asDict())\n",
        "    parquet_mean = results['avg(rating)']\n",
        "    spark_mean_time = time.time() - start_time\n",
        "\n",
        "    # Exibindo as informações\n",
        "    print(\"Rating medio no %s usando pandas: %s\" % (dataset, round(pandas_mean, num_casas)))\n",
        "    print(\"Rating medio no %s usando pandas: %s\" % (dataset, round(pyspark_mean, num_casas)))\n",
        "    print(\"Rating medio no %s usando pandas: %s\" % (dataset, round(parquet_mean, num_casas)))\n",
        "    print(\"\\n\")\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando pandas: %s segundos\" % (dataset, round(pandas_mean_time, num_casas)))\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando PySpark: %s segundos\" % (dataset, round(pyspark_mean_time, num_casas)))\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando Spark com Parquet: %s segundos\" % (dataset, round(spark_mean_time, num_casas)))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Encontrando a biblioteca com menor tempo de processamento\n",
        "    min_time = min(pandas_mean_time, pyspark_mean_time, spark_mean_time)\n",
        "    if min_time == pandas_mean_time:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o Pandas\" % dataset)\n",
        "    elif min_time == pyspark_mean_time:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o PySpark\" % dataset)\n",
        "    else:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o Spark com Parquet\" % dataset)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    \n",
        "\n",
        "# aplicando a função para Netflix_Movie_Data\n",
        "mean_rating_time(\"Netflix_Movie_Data\", df_Netflix_pandas, df_Netflix_spark, df_Netflix_parquet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6EEvRfEddO1"
      },
      "source": [
        "#### Mean amount on Fraudulent_Transactions_Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV_uKEyRdMBk",
        "outputId": "0bc95f83-aecc-4194-f287-364a39e7589d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount medio no Fraudulent_Transactions_Data usando pandas: 6362620\n",
            "Amount medio no Fraudulent_Transactions_Data usando pandas: 179861.90355\n",
            "Amount medio no Fraudulent_Transactions_Data usando pandas: 179861.90355\n",
            "\n",
            "\n",
            "Tempo de calculo da médias do dataset Fraudulent_Transactions_Data usando pandas: 0.03034 segundos\n",
            "Tempo de calculo da médias do dataset Fraudulent_Transactions_Data usando PySpark: 11.61385 segundos\n",
            "Tempo de calculo da médias do dataset Fraudulent_Transactions_Data usando Spark com Parquet: 0.68237 segundos\n",
            "\n",
            "\n",
            "A biblioteca com menor tempo de processamento para o dataset Fraudulent_Transactions_Data é o Pandas\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "# Função para calcular o valor médio do ranting \n",
        "def mean_amount_time(dataset, df_pandas, df_spark, df_parquet):\n",
        "    \"\"\"\n",
        "    Função que compara o tempo de processamento entre as bibliotecas Pandas, PySpark e Spark com Parquet ao contar as linhas de um determinado dataset.\n",
        "\n",
        "    Parâmetros:\n",
        "    dataset (str) : Nome do dataset a ser avaliado.\n",
        "    df_pandas (pandas.DataFrame) : DataFrame do Pandas para o dataset informado.\n",
        "    df_spark (pyspark.sql.DataFrame) : DataFrame do PySpark para o dataset informado.\n",
        "    df_parquet (pyspark.sql.DataFrame) : DataFrame do Spark com Parquet para o dataset informado.\n",
        "\n",
        "    Saídas:\n",
        "    Imprime na tela o número médio de cada biblioteca para o dataset informado e o tempo de processamento de contar as linhas. Além disso, imprime a biblioteca com menor tempo de processamento para o dataset informado.\n",
        "    \"\"\"\n",
        "    #numero de casas de arredondamento\n",
        "    num_casas = 5\n",
        "    \n",
        "    start_time = time.time()\n",
        "    pandas_mean = df_pandas[df_pandas.columns[2]].count()\n",
        "    pandas_mean_time = time.time() - start_time\n",
        "    \n",
        "    start_time = time.time()\n",
        "    pyspark_mean = df_spark.select(mean('amount')).collect()\n",
        "    results={}\n",
        "    for i in pyspark_mean:\n",
        "      results.update(i.asDict())\n",
        "    pyspark_mean = results['avg(amount)']\n",
        "    pyspark_mean_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    parquet_mean = df_parquet.select(mean('amount')).collect()\n",
        "    results={}\n",
        "    for i in parquet_mean:\n",
        "      results.update(i.asDict())\n",
        "    parquet_mean = results['avg(amount)']\n",
        "    spark_mean_time = time.time() - start_time\n",
        "\n",
        "    # Exibindo as informações\n",
        "    print(\"Amount medio no %s usando pandas: %s\" % (dataset, round(pandas_mean, num_casas)))\n",
        "    print(\"Amount medio no %s usando pandas: %s\" % (dataset, round(pyspark_mean, num_casas)))\n",
        "    print(\"Amount medio no %s usando pandas: %s\" % (dataset, round(parquet_mean, num_casas)))\n",
        "    print(\"\\n\")\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando pandas: %s segundos\" % (dataset, round(pandas_mean_time, num_casas)))\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando PySpark: %s segundos\" % (dataset, round(pyspark_mean_time, num_casas)))\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando Spark com Parquet: %s segundos\" % (dataset, round(spark_mean_time, num_casas)))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Encontrando a biblioteca com menor tempo de processamento\n",
        "    min_time = min(pandas_mean_time, pyspark_mean_time, spark_mean_time)\n",
        "    if min_time == pandas_mean_time:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o Pandas\" % dataset)\n",
        "    elif min_time == pyspark_mean_time:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o PySpark\" % dataset)\n",
        "    else:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o Spark com Parquet\" % dataset)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    \n",
        "# aplicando a função para Fraudulent_Transactions_Data\n",
        "mean_amount_time(\"Fraudulent_Transactions_Data\", df_Fraud_pandas, df_Fraud_spark, df_Fraud_parquet)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contar distintos"
      ],
      "metadata": {
        "id": "0gNdTTT1Z_HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Contar distintos netflix - Netflix_Movie_Data"
      ],
      "metadata": {
        "id": "YJMXpqJOaEL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "# Função para calcular o valor médio do ranting \n",
        "def diff_actor_time(dataset, df_pandas, df_spark, df_parquet):\n",
        "    \"\"\"\n",
        "    Função que compara o tempo de processamento entre as bibliotecas Pandas, PySpark e Spark com Parquet ao contar as linhas diferentes\n",
        "    Parâmetros:\n",
        "    dataset (str) : Nome do dataset a ser avaliado.\n",
        "    df_pandas (pandas.DataFrame) : DataFrame do Pandas para o dataset informado.\n",
        "    df_spark (pyspark.sql.DataFrame) : DataFrame do PySpark para o dataset informado.\n",
        "    df_parquet (pyspark.sql.DataFrame) : DataFrame do Spark com Parquet para o dataset informado.\n",
        "\n",
        "    Saídas:\n",
        "    Imprime na tela o número de linhas diferentes de cada biblioteca para o dataset informado e o tempo de processamento de contar as linhas. Além disso, imprime a biblioteca com menor tempo de processamento para o dataset informado.\n",
        "    \"\"\"\n",
        "    #numero de casas de arredondamento\n",
        "    num_casas = 5\n",
        "    \n",
        "    start_time = time.time()\n",
        "    pandas_diff = len(df_pandas[\"actors\"].unique())\n",
        "    pandas_diff_time = time.time() - start_time\n",
        "    \n",
        "    start_time = time.time()\n",
        "    pyspark_diff = df_spark.select(countDistinct('actors')).collect()\n",
        "    results={}\n",
        "    for i in pyspark_diff:\n",
        "      results.update(i.asDict())\n",
        "    pyspark_diff = results['count(DISTINCT actors)']\n",
        "    pyspark_diff_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    spark_diff = df_parquet.select(countDistinct('actors')).collect()\n",
        "    results={}\n",
        "    for i in spark_diff:\n",
        "      results.update(i.asDict())\n",
        "    spark_diff = results['count(DISTINCT actors)']\n",
        "    spark_diff_time = time.time() - start_time\n",
        "\n",
        "    # Exibindo as informações\n",
        "    print(\"Contagem diferentes actors no %s usando pandas: %s\" % (dataset, round(pandas_diff, num_casas)))\n",
        "    print(\"Contagem diferentes actors no %s usando pandas: %s\" % (dataset, round(pyspark_diff, num_casas)))\n",
        "    print(\"Contagem diferentes actors no %s usando pandas: %s\" % (dataset, round(spark_diff, num_casas)))\n",
        "    print(\"\\n\")\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando pandas: %s segundos\" % (dataset, round(pandas_diff_time, num_casas)))\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando PySpark: %s segundos\" % (dataset, round(pyspark_diff_time, num_casas)))\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando Spark com Parquet: %s segundos\" % (dataset, round(spark_diff_time, num_casas)))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Encontrando a biblioteca com menor tempo de processamento\n",
        "    min_time = min(pandas_diff_time, pyspark_diff_time, spark_diff_time)\n",
        "    if min_time == pandas_diff_time:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o Pandas\" % dataset)\n",
        "    elif min_time == pyspark_diff_time:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o PySpark\" % dataset)\n",
        "    else:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o Spark com Parquet\" % dataset)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    \n",
        "\n",
        "# aplicando a função para Netflix_Movie_Data\n",
        "diff_actor_time(\"Netflix_Movie_Data\", df_Netflix_pandas, df_Netflix_spark, df_Netflix_parquet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-tHt3qrbuoQ",
        "outputId": "16102990-a751-4624-8914-25167afad215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contagem diferentes actors no Netflix_Movie_Data usando pandas: 3237\n",
            "Contagem diferentes actors no Netflix_Movie_Data usando pandas: 3237\n",
            "Contagem diferentes actors no Netflix_Movie_Data usando pandas: 3237\n",
            "\n",
            "\n",
            "Tempo de calculo da médias do dataset Netflix_Movie_Data usando pandas: 0.01248 segundos\n",
            "Tempo de calculo da médias do dataset Netflix_Movie_Data usando PySpark: 2.28388 segundos\n",
            "Tempo de calculo da médias do dataset Netflix_Movie_Data usando Spark com Parquet: 0.52489 segundos\n",
            "\n",
            "\n",
            "A biblioteca com menor tempo de processamento para o dataset Netflix_Movie_Data é o Pandas\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Contar distintos - Fraudulent_Transactions_Data"
      ],
      "metadata": {
        "id": "b7nf_h-zaHku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "# Função para calcular o valor médio do ranting \n",
        "def diff_nameOrig_time(dataset, df_pandas, df_spark, df_parquet):\n",
        "    \"\"\"\n",
        "    Função que compara o tempo de processamento entre as bibliotecas Pandas, PySpark e Spark com Parquet ao contar as linhas diferentes\n",
        "    Parâmetros:\n",
        "    dataset (str) : Nome do dataset a ser avaliado.\n",
        "    df_pandas (pandas.DataFrame) : DataFrame do Pandas para o dataset informado.\n",
        "    df_spark (pyspark.sql.DataFrame) : DataFrame do PySpark para o dataset informado.\n",
        "    df_parquet (pyspark.sql.DataFrame) : DataFrame do Spark com Parquet para o dataset informado.\n",
        "\n",
        "    Saídas:\n",
        "    Imprime na tela o número de linhas diferentes de cada biblioteca para o dataset informado e o tempo de processamento de contar as linhas. Além disso, imprime a biblioteca com menor tempo de processamento para o dataset informado.\n",
        "    \"\"\"\n",
        "    #numero de casas de arredondamento\n",
        "    num_casas = 5\n",
        "    \n",
        "    start_time = time.time()\n",
        "    pandas_diff = len(df_pandas['originating_customer'].unique())\n",
        "    pandas_diff_time = time.time() - start_time\n",
        "    \n",
        "    start_time = time.time()\n",
        "    pyspark_diff = df_spark.select(countDistinct('originating_customer'))\n",
        "    results={}\n",
        "    for i in pyspark_diff:\n",
        "      results.update(i.asDict())\n",
        "    pyspark_diff = results['count(DISTINCT originating_customer)']\n",
        "    pyspark_diff_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    spark_diff = df_parquet.select(countDistinct('originating_customer'))\n",
        "    results={}\n",
        "    for i in spark_diff:\n",
        "      results.update(i.asDict())\n",
        "    spark_diff = results['count(DISTINCT originating_customer)']\n",
        "    spark_diff_time = time.time() - start_time\n",
        "\n",
        "    # Exibindo as informações\n",
        "    print(\"Contagem diferentes customer_origim no %s usando pandas: %s\" % (dataset, round(pandas_diff, num_casas)))\n",
        "    print(\"Contagem diferentes customer_origim no %s usando pandas: %s\" % (dataset, round(pyspark_diff, num_casas)))\n",
        "    print(\"Contagem diferentes customer_origim no %s usando pandas: %s\" % (dataset, round(spark_diff, num_casas)))\n",
        "    print(\"\\n\")\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando pandas: %s segundos\" % (dataset, round(pandas_diff_time, num_casas)))\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando PySpark: %s segundos\" % (dataset, round(pyspark_diff_time, num_casas)))\n",
        "    print(\"Tempo de calculo da médias do dataset %s usando Spark com Parquet: %s segundos\" % (dataset, round(spark_diff_time, num_casas)))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Encontrando a biblioteca com menor tempo de processamento\n",
        "    min_time = min(pandas_diff_time, pyspark_diff_time, spark_diff_time)\n",
        "    if min_time == pandas_diff_time:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o Pandas\" % dataset)\n",
        "    elif min_time == pyspark_diff_time:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o PySpark\" % dataset)\n",
        "    else:\n",
        "        print(\"A biblioteca com menor tempo de processamento para o dataset %s é o Spark com Parquet\" % dataset)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    \n",
        "\n",
        "# aplicando a função para Netflix_Movie_Data\n",
        "diff_nameOrig_time(\"Fraudulent_Transactions_Data\", df_Fraud_pandas, df_Fraud_spark, df_Fraud_parquet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "mjHs1hl5fs24",
        "outputId": "00bedd61-d26b-4031-d8a5-88d8f10621a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-60340f8f9837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# aplicando a função para Netflix_Movie_Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mdiff_nameOrig_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fraudulent_Transactions_Data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_Fraud_pandas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_Fraud_spark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_Fraud_parquet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-60340f8f9837>\u001b[0m in \u001b[0;36mdiff_nameOrig_time\u001b[0;34m(dataset, df_pandas, df_spark, df_parquet)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpyspark_diff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mpyspark_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'count(DISTINCT originating_customer)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpyspark_diff_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VWo6nfR2qyvJ",
        "yjvOeKsBEkbE",
        "WTExanCAIdK1",
        "kTxERahtIXEr",
        "nzTK7ZR1Is6v",
        "PFV2m8_nK4ET"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}